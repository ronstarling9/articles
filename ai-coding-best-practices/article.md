AI Won't Fix Your Broken Engineering Culture. It'll Expose It.

Everyone's asking: "What productivity gains are you seeing from AI coding tools?"

But that's the wrong question.

The right question: What did your engineering practices look like *before* you adopted these tools?

After years of transforming software engineering organizations and watching dozens of SaaS companies adopt agentic AI, I'll make a prediction: The gap between disciplined and undisciplined engineering teams is about to become a chasm.

AI coding tools are amplifiers. Companies with mature practices—test-driven development, disciplined defect tracking, documented coding standards—will see those practices supercharged. Companies without them will generate code faster than ever. And technical debt faster than ever.

The research backs this up. A recent study of Cursor adoption (https://arxiv.org/html/2511.04427) found initial velocity gains of 3-5x—but those gains dissipated within two months. What persisted? A 30% increase in static analysis warnings and 41% jump in code complexity.

Meanwhile, METR's 2025 study (https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) found that experienced developers were actually 19% slower with AI tools. The difference? Context and discipline. Developers who knew their codebases and had rigorous practices could direct the AI effectively. Those who didn't spent more time fixing AI-generated mistakes than they saved.

This isn't a tool problem. It's a process problem.

And here's the uncomfortable question most leaders can't answer: How do you know your engineering department is actually improving? Not just shipping faster—but shipping better? Most software organizations lack the basic instrumentation to measure. If you can't track defect escape rates, cycle time, or rework percentages today, AI adoption won't give you visibility. It'll just give you faster chaos.

TDD, clear requirements, documented standards—these aren't bureaucratic overhead. They're the guardrails that keep AI productivity real instead of illusory. Even Anthropic's own engineering guidance (https://www.anthropic.com/engineering/claude-code-best-practices) for Claude Code emphasizes test-driven development as essential.

So I'm curious: What's your experience?

Are you seeing sustained productivity gains from AI coding tools—or an initial spike that's quietly creating cleanup work? And be honest: do you have the metrics to actually know the difference?
